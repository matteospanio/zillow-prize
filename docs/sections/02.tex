The measure I took into account to evaluate the model is the mean absolute error, since it was the measure used by Zillow to evaluate the models submitted to the competition. I think that the mean squared error would have been better, it penalizes more big errors while it reduces the weigth of small errors, but I didn't want to change the metric used by Zillow, so I decided to stick with the mean absolute error.

%\subsection{Base models}

As a base model I used the mean, since the $\log error$ is normally distributed its mean is the value we can expect with most probability. Making predictions by county or with one generalized model, there is evidence that the county matters, it could be that a distance based model can do well for this task, so I made an attempt with a KNN model, but it didn't perform well, with a $5$-folded grid search cross validation the model was the model estimations improves as $k$ increases, it means that the model would probably converge to the mean if we kept using bigger and bigger $k$.

%\subsection{Keep it simple: linear regression and decision trees}

Successively I kept things as simple as possible, I didn't expect to get better results, but wanted to try the linear regressor and a decision tree, those methods aren't expected to explain main part of the errors' variance, but if they get a little better result than the base model it would be a good sign and maybe we can infer something about the data. With the linear model it came out that features about house's dimension are the most important and we get better predictions without using other informations (even if the difference is quite small, with a significance of the fourth digit after the decimal point). The decision tree model instead wasn't even useful for making parameter inference: a shallow tree is not able to make a reliable classification of the features because it does not make enough decisions on the data, furthermore from cross validation the best results were obtained with the random split criterion so the features importance is not reliable.

%\subsection{Ensemble methods}

%TODO
The ensemble methods are the most interesting part of this work, I tried to use the most common methods, the random forest and the gradient boosting, I also tried to use the XGBoost library, but I didn't get better results than the scikit-learn implementation, so I decided to stick with the latter. The random forest model is the one that performs better, it is able to explain a little more than the $10\%$ of the variance of the $\log error$ and it is able to make a reliable inference about the features importance. The gradient boosting model is not able to explain more than the $5\%$ of the variance of the $\log error$ and it is not able to make a reliable inference about the features importance. The random forest model is able to explain more than the $10\%$ of the variance of the $\log error$ and it is able to make a reliable inference about the features importance. The gradient boosting model is not able to explain more than the $5\%$ of the variance of the $\log error$ and it is not able to make a reliable inference about the features importance.



\subsection{Predictive quality of the fetures}\label{crafted_features}

My assumptions before conducting the analysis were that, if there is a possibility of predicting \textit{Zestimate}'s error, probably the fact that some houses had severe data gaps could lead to errors; on top of that the data delivery format in the competition asked to highlight logerror differences over time, so I added many features to keep track of the day and month of sale. Well, none of the assumptions has proved particularly well founded at the light of the analyzes conducted, in particular the temporal data have a low correlation with the error except for a few days of the week which seems to have a higher incidence of larger logerrors (which is possible, but it could also be a spurious correlation). As regards the predictive ability of counting null values per row, the random forest has classified it as not too irrelevant, even if the value of taxes and the extension of the house in square feets remain at the top.


The most interesting result is that the random forest has classified the features about the house's location as the most important, which is a good sign, since it means that the model is able to make a good use of the geographical information.