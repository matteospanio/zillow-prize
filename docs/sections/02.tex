The measure I took into account to evaluate the model is the mean absolute error, since it was the measure used by Zillow to evaluate the models submitted to the competition. I think that the mean squared error would have been better, it penalizes more big errors while it reduces the weigth of small errors, but I didn't want to change the metric used in the contest, so I decided to stick with the mean absolute error.

\subsection{Base models}

As a base model I used the mean, since the $\log error$ is normally distributed its mean is the value we can expect with most probability. Making predictions by county or with one generalized model, there is evidence that the county matters, it could be that a distance based model can do well for this task, so I made an attempt with a KNN model, but it didn't perform well, with a $5$-folded grid search cross validation the model estimations improves as $k$ increases, it means that the model would probably converge to the mean if we kept using bigger and bigger $k$.

\subsection{Keep it simple: linear regression and decision trees}

Successively I kept things as simple as possible, I didn't expect to get better results, but wanted to try the linear regressor and a decision tree, those methods aren't expected to explain main part of the errors' variance, but if they get a little better result than the base model it would be a good sign and maybe we can infer something about the data. With the linear model it came out that features about house's dimension are the most important and we get better predictions without using other informations (even if the difference is quite small, with a significance of the fourth digit after the decimal point). The decision tree model instead wasn't even useful for making parameter inference: a shallow tree is not able to make a reliable classification of the features because it does not make enough decisions on the data, furthermore from cross validation the best results were obtained with the random split criterion so the features importance is not reliable.

\subsection{Ensemble methods}

By now we are not making good predictions, the improvement with linear regression and decision trees was not really significant, we may expect to get better result by fitting a random forest. 

Surprisingly we are not able to overfit the forest: we would like to get a really low error on training data and a high error on test data, so the forest could optimize the variance but performing cross validation parameter tuning on the forest it came out that the model gets better as the number of elements for leaf increases, so the model is going in the opposite side than expected, we are still trying to converge to the mean. And we get an even worse result with boosting, the model is really unstable and we can't find any trend in parameter tuning except that we have the same performance on test and validation sets: the model is litterally learning nothing.



\subsection{Predictive quality of the fetures}\label{crafted_features}

My assumptions before conducting the analysis were that, if there is a possibility of predicting \textit{Zestimate}'s error, probably the fact that some houses had severe data gaps could lead to errors; on top of that the data delivery format in the competition asked to highlight logerror differences over time, so I added many features to keep track of the day and month of sale. Well, none of the assumptions has proved particularly well founded at the light of the analyzes conducted, in particular the temporal data have a low correlation with the error except for a few days of the week which seems to have a higher incidence of larger logerrors (which is possible, but it could also be a spurious correlation). As regards the predictive ability of counting null values per row, the random forest has classified it as not too irrelevant, even if the value of taxes and the extension of the house in square feets remain at the top.