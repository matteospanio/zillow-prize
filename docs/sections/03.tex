The \textit{Zestimate} algorithm is almost $100\%$ precise, most of the time its error is under $1\%$. The rest of errors  

$$\text{Error}(\mathcal{M} ) = \text{Bias}^2 + \text{Variance} + \varepsilon$$
Tenendo in considerazione il fatto che il boosting dovrebbe ridurre il bias e il bagging dovrebbe ridurre la varianza, ma nessuno dei due modelli ha fornito risultati buoni.

\begin{itemize}
  \item rifare togliendo le righe doppioni
  \item indagare maggiormente sugli outliers
  \item creare altre variabili rapporto tra variabili
  \item standardize $\log error$ dividing by the standard deviation
\end{itemize}

If we assume that most of the errors have been made by humans it is not a surprise to see that errors are most incident where there are more data.